{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsqW50heB8nE4srpuc4nOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/algodigger/playground/blob/main/RNN_vs_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the notebook to demonstrate differences between RNN and GRU"
      ],
      "metadata": {
        "id": "_Q1_7NL2OpY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "_9UtLVGxO1iW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers,  batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        ""
      ],
      "metadata": {
        "id": "yUQrLIi8PIYo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x, _ = self.gru(x)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "2JkC32eNSON-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmLq-eOBrRDB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate data"
      ],
      "metadata": {
        "id": "tI9_vKPArdu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(1000):\n",
        "  data.append((list(range(i, i+10)), list(range(i+1, i+1+10))))\n",
        "  # data = [(list(range(10)), list(range(1, 11))), (list(range(10, 20)), list(range(11, 21)))]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "print(data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ArNl-iZKVr",
        "outputId": "9b07c2cf-2300-4ea4-c3f3-bcf662c57731"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([674, 675, 676, 677, 678, 679, 680, 681, 682, 683], [675, 676, 677, 678, 679, 680, 681, 682, 683, 684])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_RNN_GRU(input_size, hidden_size, output_size, num_layers, lr, epochs, data):\n",
        "    # Initialize models\n",
        "    rnn_model = RNN(input_size, hidden_size, output_size, num_layers=num_layers)\n",
        "    gru_model = GRU(input_size, hidden_size, output_size, num_layers=num_layers)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=lr)\n",
        "    gru_optimizer = torch.optim.Adam(gru_model.parameters(), lr=lr)\n",
        "\n",
        "    best_rnn_model_state = None\n",
        "    best_gru_model_state = None\n",
        "    best_rnn_loss = float('inf')\n",
        "    best_gru_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        rnn_total_loss = 0\n",
        "        gru_total_loss = 0\n",
        "        for input_sequence, target in data:\n",
        "            input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
        "            target = torch.Tensor(target).view(len(target), -1)\n",
        "            rnn_optimizer.zero_grad()\n",
        "            output_seq = rnn_model(input_sequence)\n",
        "            rnn_loss = criterion(output_seq, target)\n",
        "            rnn_loss.backward()\n",
        "            rnn_optimizer.step()\n",
        "            rnn_total_loss += rnn_loss.item()\n",
        "\n",
        "            gru_optimizer.zero_grad()\n",
        "            output_seq = gru_model(input_sequence)\n",
        "            gru_loss = criterion(output_seq, target)\n",
        "            gru_loss.backward()\n",
        "            gru_optimizer.step()\n",
        "            gru_total_loss += gru_loss.item()\n",
        "\n",
        "        rnn_avg_loss = rnn_total_loss / len(data)\n",
        "        gru_avg_loss = gru_total_loss / len(data)\n",
        "\n",
        "        # Save best models based on validation loss\n",
        "        if rnn_avg_loss < best_rnn_loss:\n",
        "            best_rnn_loss = rnn_avg_loss\n",
        "            best_rnn_model_state = rnn_model.state_dict()\n",
        "\n",
        "        if gru_avg_loss < best_gru_loss:\n",
        "            best_gru_loss = gru_avg_loss\n",
        "            best_gru_model_state = gru_model.state_dict()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, RNN Loss: {rnn_avg_loss}, GRU Loss: {gru_avg_loss}\")\n",
        "\n",
        "    rnn_model.load_state_dict(best_rnn_model_state)\n",
        "    gru_model.load_state_dict(best_gru_model_state)\n",
        "\n",
        "    return rnn_model, gru_model\n"
      ],
      "metadata": {
        "id": "RMKXKd6xRmpc"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def test(model, data, criterion = nn.MSELoss()):\n",
        "  total_loss = 0\n",
        "  for input_sequence, target in data:\n",
        "    input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
        "    target = torch.Tensor(target).view(len(target), -1)\n",
        "    output = model(input_sequence)\n",
        "    total_loss += criterion(output, target).item()\n",
        "    return total_loss / len(data)\n",
        "  print(f\"Test loss is {total_loss}\")"
      ],
      "metadata": {
        "id": "f6FVlcIY1ESl"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(rnn_model, gru_model, seuqence=list(range(10, 20))):\n",
        "  input_sequence = torch.Tensor(seuqence).view(10, 1, -1)\n",
        "\n",
        "  rnn_output = rnn_model(input_sequence)\n",
        "  gru_output = gru_model(input_sequence)\n",
        "\n",
        "  # print(rnn_output)\n",
        "  # print(rnn_output)\n",
        "\n",
        "\n",
        "  print(f\"Sequence {seuqence}\")\n",
        "  print(f'RNN Predicted next number: {rnn_output[-1].item():.4f}')\n",
        "  print(f'GRU Predicted next number: {gru_output[-1].item():.4f}')\n"
      ],
      "metadata": {
        "id": "6_T7-yvm38Pa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train with different sets of hyperparams\n"
      ],
      "metadata": {
        "id": "kIwCarGFzeOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "hidden_size = 32\n",
        "output_size = 1\n",
        "number_layers = 1\n",
        "lr = 0.001\n",
        "epochs = 100\n",
        "rnn_model_l1, gru_model_l2 = train_RNN_GRU(input_size, hidden_size, output_size, number_layers, lr, epochs, data)\n",
        "predict(rnn_model_l1, gru_model_l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q2WBN9503rQ",
        "outputId": "f1787ba1-ae4a-4026-944a-efff15c0914a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, RNN Loss: 324846.49424635124, GRU Loss: 323921.0062001715\n",
            "Epoch 2, RNN Loss: 298276.3157689858, GRU Loss: 295318.265740613\n",
            "Epoch 3, RNN Loss: 273870.03074251843, GRU Loss: 270655.40552967455\n",
            "Epoch 4, RNN Loss: 251327.6367607479, GRU Loss: 248202.42705220127\n",
            "Epoch 5, RNN Loss: 230458.514427166, GRU Loss: 227499.39215038682\n",
            "Epoch 6, RNN Loss: 211124.49441254424, GRU Loss: 208339.30668241787\n",
            "Epoch 7, RNN Loss: 193189.3069345684, GRU Loss: 190577.19956401348\n",
            "Epoch 8, RNN Loss: 176548.91586934854, GRU Loss: 174118.41173921202\n",
            "Epoch 9, RNN Loss: 161135.39280129145, GRU Loss: 158899.16072394658\n",
            "Epoch 10, RNN Loss: 146890.49494703102, GRU Loss: 144824.33179019546\n",
            "Epoch 11, RNN Loss: 133707.37434609796, GRU Loss: 131801.87572152805\n",
            "Epoch 12, RNN Loss: 121559.41763469887, GRU Loss: 119807.37838124276\n",
            "Epoch 13, RNN Loss: 110366.97046214485, GRU Loss: 108765.98821656418\n",
            "Epoch 14, RNN Loss: 100051.6121596775, GRU Loss: 98588.0135194397\n",
            "Epoch 15, RNN Loss: 90617.64483088112, GRU Loss: 89277.83662037659\n",
            "Epoch 16, RNN Loss: 81895.53591809083, GRU Loss: 80679.91248188495\n",
            "Epoch 17, RNN Loss: 73885.31105051422, GRU Loss: 72774.37031257439\n",
            "Epoch 18, RNN Loss: 66622.81164543248, GRU Loss: 65552.60428277684\n",
            "Epoch 19, RNN Loss: 59949.58458580398, GRU Loss: 59003.360791423795\n",
            "Epoch 20, RNN Loss: 56092.06678266812, GRU Loss: 53038.99861545181\n",
            "Epoch 21, RNN Loss: 56136.63853554153, GRU Loss: 47622.26192007446\n",
            "Epoch 22, RNN Loss: 50399.17509549904, GRU Loss: 42617.22055864811\n",
            "Epoch 23, RNN Loss: 45075.3654929533, GRU Loss: 39764.443150510786\n",
            "Epoch 24, RNN Loss: 40335.612063090324, GRU Loss: 37497.68900125408\n",
            "Epoch 25, RNN Loss: 37912.22826561928, GRU Loss: 33490.269728108404\n",
            "Epoch 26, RNN Loss: 39839.72271017933, GRU Loss: 29918.890408158302\n",
            "Epoch 27, RNN Loss: 37569.32330169964, GRU Loss: 32006.604963523863\n",
            "Epoch 28, RNN Loss: 41649.027368371964, GRU Loss: 46334.2621878109\n",
            "Epoch 29, RNN Loss: 42875.003865184786, GRU Loss: 46590.1793544178\n",
            "Epoch 30, RNN Loss: 41209.666753167156, GRU Loss: 42028.26925629329\n",
            "Epoch 31, RNN Loss: 42937.22087644577, GRU Loss: 37957.222492991445\n",
            "Epoch 32, RNN Loss: 41428.968364198685, GRU Loss: 34278.44893906212\n",
            "Epoch 33, RNN Loss: 43531.80565104389, GRU Loss: 31002.4220975523\n",
            "Epoch 34, RNN Loss: 38831.16965857029, GRU Loss: 27957.92223660469\n",
            "Epoch 35, RNN Loss: 37746.09811560249, GRU Loss: 27726.870202317237\n",
            "Epoch 36, RNN Loss: 40399.23195768452, GRU Loss: 30499.412648898124\n",
            "Epoch 37, RNN Loss: 39626.891047976496, GRU Loss: 27433.40776294899\n",
            "Epoch 38, RNN Loss: 42584.841707323074, GRU Loss: 27540.558687702178\n",
            "Epoch 39, RNN Loss: 37842.56821618461, GRU Loss: 30736.778823482513\n",
            "Epoch 40, RNN Loss: 37223.43356638336, GRU Loss: 27755.80444485855\n",
            "Epoch 41, RNN Loss: 40819.926449578285, GRU Loss: 28216.14298347759\n",
            "Epoch 42, RNN Loss: 36590.019274374965, GRU Loss: 32064.201385382654\n",
            "Epoch 43, RNN Loss: 32453.819491197584, GRU Loss: 29035.548381346704\n",
            "Epoch 44, RNN Loss: 28811.111672683717, GRU Loss: 26210.159544075967\n",
            "Epoch 45, RNN Loss: 25515.766195498465, GRU Loss: 23709.035885513305\n",
            "Epoch 46, RNN Loss: 22628.498247128486, GRU Loss: 24598.742940973283\n",
            "Epoch 47, RNN Loss: 21437.46373166847, GRU Loss: 28788.035003460885\n",
            "Epoch 48, RNN Loss: 25510.27553734398, GRU Loss: 25958.286963149072\n",
            "Epoch 49, RNN Loss: 17579.909485884666, GRU Loss: 23437.01290736866\n",
            "Epoch 50, RNN Loss: 15309.386500157356, GRU Loss: 24589.254792806627\n",
            "Epoch 51, RNN Loss: 13877.159405132294, GRU Loss: 29259.609943152427\n",
            "Epoch 52, RNN Loss: 12016.037849849701, GRU Loss: 26388.860415781975\n",
            "Epoch 53, RNN Loss: 12999.787761921883, GRU Loss: 23794.269223317147\n",
            "Epoch 54, RNN Loss: 14685.415757092476, GRU Loss: 21512.281418088915\n",
            "Epoch 55, RNN Loss: 12963.163735817909, GRU Loss: 19424.459376983643\n",
            "Epoch 56, RNN Loss: 11293.59416833687, GRU Loss: 17529.026838944435\n",
            "Epoch 57, RNN Loss: 11535.531579525948, GRU Loss: 15802.53461941433\n",
            "Epoch 58, RNN Loss: 16414.950351027488, GRU Loss: 14174.551737451553\n",
            "Epoch 59, RNN Loss: 20052.61031572914, GRU Loss: 12730.607757371903\n",
            "Epoch 60, RNN Loss: 17826.509346632003, GRU Loss: 11440.879026835442\n",
            "Epoch 61, RNN Loss: 18120.0912777853, GRU Loss: 10265.495155342101\n",
            "Epoch 62, RNN Loss: 23365.791152466772, GRU Loss: 9216.69138494873\n",
            "Epoch 63, RNN Loss: 20587.883490850447, GRU Loss: 8303.651192975045\n",
            "Epoch 64, RNN Loss: 18203.7321162529, GRU Loss: 7406.854872643471\n",
            "Epoch 65, RNN Loss: 16104.66951896286, GRU Loss: 6670.038076712608\n",
            "Epoch 66, RNN Loss: 14205.767373841285, GRU Loss: 5884.634337400436\n",
            "Epoch 67, RNN Loss: 12453.142677648544, GRU Loss: 5310.490129659653\n",
            "Epoch 68, RNN Loss: 15047.48660562229, GRU Loss: 4858.735037063599\n",
            "Epoch 69, RNN Loss: 13580.66012469387, GRU Loss: 4297.844995415688\n",
            "Epoch 70, RNN Loss: 11756.739389740944, GRU Loss: 3858.2434380712507\n",
            "Epoch 71, RNN Loss: 11190.402130290031, GRU Loss: 3854.584569820404\n",
            "Epoch 72, RNN Loss: 18186.501660606384, GRU Loss: 7522.775872800827\n",
            "Epoch 73, RNN Loss: 34081.62442426109, GRU Loss: 6780.924386345863\n",
            "Epoch 74, RNN Loss: 29821.133499367716, GRU Loss: 6046.211790356636\n",
            "Epoch 75, RNN Loss: 26439.157746629717, GRU Loss: 5502.026606884003\n",
            "Epoch 76, RNN Loss: 23332.559741796493, GRU Loss: 5085.970401002884\n",
            "Epoch 77, RNN Loss: 20680.628310248376, GRU Loss: 5328.096924683571\n",
            "Epoch 78, RNN Loss: 21621.060866125106, GRU Loss: 3944.210110823631\n",
            "Epoch 79, RNN Loss: 30243.519721117973, GRU Loss: 3604.8744760780332\n",
            "Epoch 80, RNN Loss: 26591.183278946875, GRU Loss: 4138.748118088723\n",
            "Epoch 81, RNN Loss: 23555.859302108765, GRU Loss: 4897.316952299118\n",
            "Epoch 82, RNN Loss: 25972.358415821076, GRU Loss: 4416.796671175956\n",
            "Epoch 83, RNN Loss: 26708.5893823719, GRU Loss: 3979.342985841751\n",
            "Epoch 84, RNN Loss: 23699.60993594265, GRU Loss: 3601.7024925374985\n",
            "Epoch 85, RNN Loss: 21092.331564759254, GRU Loss: 5318.631606543541\n",
            "Epoch 86, RNN Loss: 20608.012137750626, GRU Loss: 6323.2157943115235\n",
            "Epoch 87, RNN Loss: 26204.83595667839, GRU Loss: 3260.9390035743713\n",
            "Epoch 88, RNN Loss: 21937.486773781777, GRU Loss: 3552.014703819275\n",
            "Epoch 89, RNN Loss: 19332.16092145443, GRU Loss: 3938.2482077188492\n",
            "Epoch 90, RNN Loss: 22473.633472088815, GRU Loss: 3569.9893420200347\n",
            "Epoch 91, RNN Loss: 30919.008101555824, GRU Loss: 3275.4739108104704\n",
            "Epoch 92, RNN Loss: 27215.803972715377, GRU Loss: 3499.513729077339\n",
            "Epoch 93, RNN Loss: 24070.90158382034, GRU Loss: 4999.240489845276\n",
            "Epoch 94, RNN Loss: 21418.778753294944, GRU Loss: 4617.150301200867\n",
            "Epoch 95, RNN Loss: 18896.05418762493, GRU Loss: 4183.229170912743\n",
            "Epoch 96, RNN Loss: 18074.46283217621, GRU Loss: 4069.0843548173902\n",
            "Epoch 97, RNN Loss: 20556.9062660017, GRU Loss: 6205.4378687238695\n",
            "Epoch 98, RNN Loss: 18059.482445763588, GRU Loss: 5578.385051520348\n",
            "Epoch 99, RNN Loss: 16157.393788355826, GRU Loss: 5048.866494545937\n",
            "Epoch 100, RNN Loss: 14254.50483135891, GRU Loss: 4574.963531504631\n",
            "Sequence [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "RNN Predicted next number: 15.3793\n",
            "GRU Predicted next number: 19.4795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "hidden_size = 32\n",
        "output_size = 1\n",
        "number_layers = 5\n",
        "lr = 0.01\n",
        "epochs = 100\n",
        "rnn_model_l5, gru_model_l5 = train_RNN_GRU(input_size, hidden_size, output_size, number_layers, lr, epochs, data)\n",
        "predict(rnn_model_l5, gru_model_l5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8mZDzhh1zGR",
        "outputId": "edca59b3-89d7-4f4b-ab5a-d4e4a3ad334e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, RNN Loss: 247270.5941183157, GRU Loss: 226722.547326581\n",
            "Epoch 2, RNN Loss: 126064.52194473839, GRU Loss: 117949.37899271488\n",
            "Epoch 3, RNN Loss: 91461.65756077194, GRU Loss: 259824.19182653047\n",
            "Epoch 4, RNN Loss: 84566.93563735866, GRU Loss: 313696.14662215137\n",
            "Epoch 5, RNN Loss: 83577.81072402191, GRU Loss: 305167.4303747692\n",
            "Epoch 6, RNN Loss: 83466.45307060433, GRU Loss: 297163.34994507313\n",
            "Epoch 7, RNN Loss: 83456.52640641308, GRU Loss: 289451.19838191033\n",
            "Epoch 8, RNN Loss: 83456.18189370251, GRU Loss: 281957.446958395\n",
            "Epoch 9, RNN Loss: 83456.37611361886, GRU Loss: 274656.01950138283\n",
            "Epoch 10, RNN Loss: 83456.47313938999, GRU Loss: 267536.6931286602\n",
            "Epoch 11, RNN Loss: 83456.50681081772, GRU Loss: 260595.29507275962\n",
            "Epoch 12, RNN Loss: 83456.51857797241, GRU Loss: 253829.47724011232\n",
            "Epoch 13, RNN Loss: 83456.52030560684, GRU Loss: 247237.83882396508\n",
            "Epoch 14, RNN Loss: 83456.52202852344, GRU Loss: 240818.95086821652\n",
            "Epoch 15, RNN Loss: 83456.52110721874, GRU Loss: 234571.50835905172\n",
            "Epoch 16, RNN Loss: 83456.51999753094, GRU Loss: 228494.2034604244\n",
            "Epoch 17, RNN Loss: 83456.5201691761, GRU Loss: 222585.60167045498\n",
            "Epoch 18, RNN Loss: 83456.51999797345, GRU Loss: 216844.1418454485\n",
            "Epoch 19, RNN Loss: 83456.51988812923, GRU Loss: 211268.5437698946\n",
            "Epoch 20, RNN Loss: 83456.52004554844, GRU Loss: 205857.3271938915\n",
            "Epoch 21, RNN Loss: 83456.52067365836, GRU Loss: 200608.6907473402\n",
            "Epoch 22, RNN Loss: 83456.52047864723, GRU Loss: 195520.9839190674\n",
            "Epoch 23, RNN Loss: 83456.52044190025, GRU Loss: 190592.18799544525\n",
            "Epoch 24, RNN Loss: 83456.52041710854, GRU Loss: 185820.63609816646\n",
            "Epoch 25, RNN Loss: 83456.52031810951, GRU Loss: 181204.50631548406\n",
            "Epoch 26, RNN Loss: 83456.52014654159, GRU Loss: 176741.68933510588\n",
            "Epoch 27, RNN Loss: 83456.52050542831, GRU Loss: 172429.78028068924\n",
            "Epoch 28, RNN Loss: 83456.52058098983, GRU Loss: 168266.93388640977\n",
            "Epoch 29, RNN Loss: 83456.52065624618, GRU Loss: 164250.76478705215\n",
            "Epoch 30, RNN Loss: 83456.52083452606, GRU Loss: 160378.94800660992\n",
            "Epoch 31, RNN Loss: 83456.52070055009, GRU Loss: 156649.00843662643\n",
            "Epoch 32, RNN Loss: 83456.520689744, GRU Loss: 153058.4100795555\n",
            "Epoch 33, RNN Loss: 83456.52056340122, GRU Loss: 149604.53791303255\n",
            "Epoch 34, RNN Loss: 83456.5205004263, GRU Loss: 146284.79929129983\n",
            "Epoch 35, RNN Loss: 83456.52068833828, GRU Loss: 143096.63896830176\n",
            "Epoch 36, RNN Loss: 83456.52079228115, GRU Loss: 140036.82268040086\n",
            "Epoch 37, RNN Loss: 83456.52059477997, GRU Loss: 137102.69480573465\n",
            "Epoch 38, RNN Loss: 83456.52050472259, GRU Loss: 134291.34092126082\n",
            "Epoch 39, RNN Loss: 83456.52079542541, GRU Loss: 131599.69054470444\n",
            "Epoch 40, RNN Loss: 83456.52062881851, GRU Loss: 129024.92634442425\n",
            "Epoch 41, RNN Loss: 83456.5206353798, GRU Loss: 126563.59823937702\n",
            "Epoch 42, RNN Loss: 83456.52081408883, GRU Loss: 124212.63473273277\n",
            "Epoch 43, RNN Loss: 83456.5207880497, GRU Loss: 121969.1643625431\n",
            "Epoch 44, RNN Loss: 83456.52029025841, GRU Loss: 119829.59317673302\n",
            "Epoch 45, RNN Loss: 83456.51993500805, GRU Loss: 117790.6236913395\n",
            "Epoch 46, RNN Loss: 83456.51980208588, GRU Loss: 115849.30856516647\n",
            "Epoch 47, RNN Loss: 83456.51979548646, GRU Loss: 114002.40290157509\n",
            "Epoch 48, RNN Loss: 83456.51981075668, GRU Loss: 112246.64734579468\n",
            "Epoch 49, RNN Loss: 83456.51987029552, GRU Loss: 110578.70491043282\n",
            "Epoch 50, RNN Loss: 83456.51990845012, GRU Loss: 108995.39228135109\n",
            "Epoch 51, RNN Loss: 83456.51984906674, GRU Loss: 107493.3613580637\n",
            "Epoch 52, RNN Loss: 83456.52004554844, GRU Loss: 106069.53763890266\n",
            "Epoch 53, RNN Loss: 83456.52067365836, GRU Loss: 104720.79537260246\n",
            "Epoch 54, RNN Loss: 83456.52047864723, GRU Loss: 103444.21072307206\n",
            "Epoch 55, RNN Loss: 83456.52044190025, GRU Loss: 102236.47757779693\n",
            "Epoch 56, RNN Loss: 83456.52041710854, GRU Loss: 101094.69971138763\n",
            "Epoch 57, RNN Loss: 83456.52031810951, GRU Loss: 100015.92974222565\n",
            "Epoch 58, RNN Loss: 83456.52014654159, GRU Loss: 98997.24758069801\n",
            "Epoch 59, RNN Loss: 83456.52050542831, GRU Loss: 98036.02831589317\n",
            "Epoch 60, RNN Loss: 83456.52058098983, GRU Loss: 97129.3722310629\n",
            "Epoch 61, RNN Loss: 83456.52065624618, GRU Loss: 96274.70584426212\n",
            "Epoch 62, RNN Loss: 83456.52083452606, GRU Loss: 95469.52945440578\n",
            "Epoch 63, RNN Loss: 83456.52070055009, GRU Loss: 94711.41301886464\n",
            "Epoch 64, RNN Loss: 83456.520689744, GRU Loss: 93997.82284335994\n",
            "Epoch 65, RNN Loss: 83456.52056340122, GRU Loss: 93326.53507113743\n",
            "Epoch 66, RNN Loss: 83456.5205004263, GRU Loss: 92695.27371146012\n",
            "Epoch 67, RNN Loss: 83456.52068833828, GRU Loss: 92101.93557056999\n",
            "Epoch 68, RNN Loss: 83456.52079228115, GRU Loss: 91544.48361907482\n",
            "Epoch 69, RNN Loss: 83456.52059477997, GRU Loss: 91020.93608721733\n",
            "Epoch 70, RNN Loss: 83456.52050472259, GRU Loss: 90529.45243734455\n",
            "Epoch 71, RNN Loss: 83456.52079542541, GRU Loss: 90068.26544917202\n",
            "Epoch 72, RNN Loss: 83456.52062881851, GRU Loss: 89635.50626900195\n",
            "Epoch 73, RNN Loss: 83456.5206353798, GRU Loss: 89229.60746096325\n",
            "Epoch 74, RNN Loss: 83456.52081408883, GRU Loss: 88849.09997163963\n",
            "Epoch 75, RNN Loss: 83456.5207880497, GRU Loss: 88492.44117488003\n",
            "Epoch 76, RNN Loss: 83456.52029025841, GRU Loss: 88158.17950397778\n",
            "Epoch 77, RNN Loss: 83456.51993500805, GRU Loss: 87845.02575236511\n",
            "Epoch 78, RNN Loss: 83456.51980208588, GRU Loss: 87551.73037538529\n",
            "Epoch 79, RNN Loss: 83456.51979548646, GRU Loss: 87277.11192625904\n",
            "Epoch 80, RNN Loss: 83456.51981075668, GRU Loss: 87019.99099010562\n",
            "Epoch 81, RNN Loss: 83456.51987029552, GRU Loss: 86779.33916248131\n",
            "Epoch 82, RNN Loss: 83456.51990845012, GRU Loss: 86554.1324138422\n",
            "Epoch 83, RNN Loss: 83456.51984906674, GRU Loss: 86343.43146654702\n",
            "Epoch 84, RNN Loss: 83456.52004554844, GRU Loss: 86146.31657527828\n",
            "Epoch 85, RNN Loss: 83456.52067365836, GRU Loss: 85961.98316958427\n",
            "Epoch 86, RNN Loss: 83456.52047864723, GRU Loss: 85789.58540904999\n",
            "Epoch 87, RNN Loss: 83456.52044190025, GRU Loss: 85628.39858175945\n",
            "Epoch 88, RNN Loss: 83456.52041710854, GRU Loss: 85477.67811138439\n",
            "Epoch 89, RNN Loss: 83456.52031810951, GRU Loss: 85336.7590777893\n",
            "Epoch 90, RNN Loss: 83456.52014654159, GRU Loss: 85205.05887050247\n",
            "Epoch 91, RNN Loss: 83456.52050542831, GRU Loss: 85081.97299230003\n",
            "Epoch 92, RNN Loss: 83456.52058098983, GRU Loss: 84966.94787572575\n",
            "Epoch 93, RNN Loss: 83456.52065624618, GRU Loss: 84859.48171298314\n",
            "Epoch 94, RNN Loss: 83456.52083452606, GRU Loss: 84759.03434257889\n",
            "Epoch 95, RNN Loss: 83456.52070055009, GRU Loss: 84665.22247915459\n",
            "Epoch 96, RNN Loss: 83456.52063603306, GRU Loss: 84577.58271095563\n",
            "Epoch 97, RNN Loss: 83456.52058859158, GRU Loss: 84495.7068956976\n",
            "Epoch 98, RNN Loss: 83456.52085420131, GRU Loss: 84419.25439289189\n",
            "Epoch 99, RNN Loss: 83456.52058846282, GRU Loss: 84347.85830562687\n",
            "Epoch 100, RNN Loss: 83456.52054626847, GRU Loss: 84281.1721252327\n",
            "Sequence [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "RNN Predicted next number: 505.3252\n",
            "GRU Predicted next number: 474.9289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# Define the model\n",
        "class NextNumberPredictor(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(NextNumberPredictor, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "# Train the model\n",
        "def train(model, data, loss_fn, optimizer, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    for input_sequence, target in data:\n",
        "      input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
        "      target = torch.Tensor(target).view(len(target), -1)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(input_sequence)\n",
        "      loss = loss_fn(output, target)\n",
        "\n",
        "      # Backward pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f\"Loss {loss}\")\n",
        "\n",
        "# Test the model\n",
        "def test(model, data, loss_fn):\n",
        "  total_loss = 0\n",
        "  for input_sequence, target in data:\n",
        "    input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
        "    target = torch.Tensor(target).view(len(target), -1)\n",
        "    output = model(input_sequence)\n",
        "    total_loss += loss_fn(output, target).item()\n",
        "    return total_loss / len(data)\n",
        "\n",
        "\n",
        "# Setup the model, data, loss function and optimizer\n",
        "model = NextNumberPredictor(1, 32, 1)\n",
        "data = []\n",
        "for i in range(1000):\n",
        "  data.append((list(range(i, i+10)), list(range(i+1, i+1+10))))\n",
        "  # data = [(list(range(10)), list(range(1, 11))), (list(range(10, 20)), list(range(11, 21)))]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "train(model, data, loss_fn, optimizer, num_epochs=100)\n",
        "\n",
        "# Use the model to make predictions\n",
        "input_sequence = torch.Tensor(list(range(10, 20))).view(10, 1, -1)\n",
        "output = model(input_sequence)\n",
        "prediction = output[-1].item()\n",
        "print(output)\n",
        "print(f'Predicted next number: {prediction:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xAY7ZPpUG4E",
        "outputId": "a1e900c7-fcf0-469e-f027-724a801f551b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 425070.625\n",
            "Loss 393745.40625\n",
            "Loss 363516.875\n",
            "Loss 335637.3125\n",
            "Loss 308894.125\n",
            "Loss 281507.6875\n",
            "Loss 256652.34375\n",
            "Loss 233518.015625\n",
            "Loss 211867.375\n",
            "Loss 191599.59375\n",
            "Loss 171252.078125\n",
            "Loss 153077.09375\n",
            "Loss 136268.6875\n",
            "Loss 120735.8125\n",
            "Loss 106401.40625\n",
            "Loss 93225.8125\n",
            "Loss 81173.3828125\n",
            "Loss 70096.2421875\n",
            "Loss 60069.15625\n",
            "Loss 51006.3984375\n",
            "Loss 42868.25390625\n",
            "Loss 35617.8671875\n",
            "Loss 29212.080078125\n",
            "Loss 23564.115234375\n",
            "Loss 18723.8828125\n",
            "Loss 14592.8037109375\n",
            "Loss 11103.7548828125\n",
            "Loss 8298.5341796875\n",
            "Loss 6108.85693359375\n",
            "Loss 4475.17138671875\n",
            "Loss 3477.737548828125\n",
            "Loss 2625.169677734375\n",
            "Loss 2127.0048828125\n",
            "Loss 2353.9716796875\n",
            "Loss 2658.392822265625\n",
            "Loss 1649.530029296875\n",
            "Loss 1414.6949462890625\n",
            "Loss 1160.3497314453125\n",
            "Loss 982.2234497070312\n",
            "Loss 1121.247314453125\n",
            "Loss 994.4168090820312\n",
            "Loss 810.0857543945312\n",
            "Loss 685.956298828125\n",
            "Loss 590.5377197265625\n",
            "Loss 900.513916015625\n",
            "Loss 778.13623046875\n",
            "Loss 635.4649047851562\n",
            "Loss 796.5457763671875\n",
            "Loss 667.1177978515625\n",
            "Loss 635.4287109375\n",
            "Loss 514.0511474609375\n",
            "Loss 909.9522094726562\n",
            "Loss 741.634765625\n",
            "Loss 1419.302978515625\n",
            "Loss 1213.0709228515625\n",
            "Loss 683.7496948242188\n",
            "Loss 580.0973510742188\n",
            "Loss 487.0741271972656\n",
            "Loss 394.89019775390625\n",
            "Loss 347.1723937988281\n",
            "Loss 271.7842102050781\n",
            "Loss 324.4766845703125\n",
            "Loss 383.1097412109375\n",
            "Loss 191.1428680419922\n",
            "Loss 121.06985473632812\n",
            "Loss 4080.20556640625\n",
            "Loss 2941.16650390625\n",
            "Loss 1945.1871337890625\n",
            "Loss 1541.145751953125\n",
            "Loss 1308.92041015625\n",
            "Loss 1137.376220703125\n",
            "Loss 969.5231323242188\n",
            "Loss 847.4222412109375\n",
            "Loss 728.2335205078125\n",
            "Loss 633.0420532226562\n",
            "Loss 515.3640747070312\n",
            "Loss 457.78192138671875\n",
            "Loss 353.9871826171875\n",
            "Loss 271.4508056640625\n",
            "Loss 345.3509826660156\n",
            "Loss 172.51882934570312\n",
            "Loss 131.6231689453125\n",
            "Loss 86.9350814819336\n",
            "Loss 43.7968635559082\n",
            "Loss 49.51560592651367\n",
            "Loss 30.662128448486328\n",
            "Loss 17.61675453186035\n",
            "Loss 22.281936645507812\n",
            "Loss 913.5424194335938\n",
            "Loss 27.70502471923828\n",
            "Loss 62.274253845214844\n",
            "Loss 22.997774124145508\n",
            "Loss 36.68345642089844\n",
            "Loss 29.674571990966797\n",
            "Loss 90.31240844726562\n",
            "Loss 45.85832977294922\n",
            "Loss 53.46173858642578\n",
            "Loss 473.5110168457031\n",
            "Loss 336.844970703125\n",
            "Loss 286.56463623046875\n",
            "tensor([[[15.9747]],\n",
            "\n",
            "        [[16.2624]],\n",
            "\n",
            "        [[15.3490]],\n",
            "\n",
            "        [[15.2476]],\n",
            "\n",
            "        [[15.4325]],\n",
            "\n",
            "        [[15.7164]],\n",
            "\n",
            "        [[16.0115]],\n",
            "\n",
            "        [[16.2757]],\n",
            "\n",
            "        [[16.4913]],\n",
            "\n",
            "        [[16.6530]]], grad_fn=<ViewBackward0>)\n",
            "Predicted next number: 16.6530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0saWOEZ_jKR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}